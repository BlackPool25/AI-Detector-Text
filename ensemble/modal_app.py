"""
AI Text Detection Ensemble - Modal Serverless Deployment

Production-ready GPU inference for the 3-component ensemble:
- DeTeCtive (contrastive learning + KNN)
- Binoculars (cross-model perplexity)
- Fast-DetectGPT (probability curvature)

Optimized for WhatsApp bots and web integration.

Deploy:
    modal deploy modal_app.py

Test locally:
    modal serve modal_app.py

Modal API v1.2+ compatible (December 2024).
"""

import modal
import os
import time
from datetime import datetime
from typing import Optional, Dict, List, Any

# =============================================================================
# Modal App Configuration
# =============================================================================

app = modal.App("ai-text-detector")

# GPU Image with all dependencies
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git")
    .pip_install(
        # Web framework (required for Modal web endpoints)
        "fastapi>=0.104.0",
        # ML dependencies
        "torch>=2.0.0",
        "transformers>=4.35.0",
        "numpy>=1.24.0",
        "faiss-cpu>=1.7.4",
        "scikit-learn>=1.3.0",
        "sentencepiece>=0.1.99",
        "scipy>=1.10.0",
        "huggingface_hub>=0.19.0",
        "tqdm>=4.60.0",
        # Document parsing
        "python-docx>=1.0.0",
        "PyPDF2>=3.0.0",
        "python-magic>=0.4.27",
    )
    .run_commands(
        # Pre-download models during build for faster cold starts
        "python -c \"from transformers import AutoModel, AutoTokenizer; "
        "AutoTokenizer.from_pretrained('gpt2'); "
        "AutoTokenizer.from_pretrained('gpt2-medium'); "
        "AutoTokenizer.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')\"",
        gpu="T4"
    )
)

# Persistent volume for caching models and database
model_volume = modal.Volume.from_name("ai-detector-cache", create_if_missing=True)


# =============================================================================
# Helper Functions
# =============================================================================

def format_whatsapp_response(result) -> Dict[str, Any]:
    """Format detection result for WhatsApp with emojis and markdown."""
    
    # Verdict emoji and text
    if result.prediction == "AI":
        verdict_emoji = "ü§ñ"
        verdict_text = "AI-Generated"
        action_line = "‚õî *Recommendation:* Likely AI content - verify source"
    elif result.prediction == "Human":
        verdict_emoji = "üë§"
        verdict_text = "Human-Written"
        action_line = "‚úÖ *Recommendation:* Appears authentic"
    else:
        verdict_emoji = "‚ùì"
        verdict_text = "Uncertain"
        action_line = "‚ö†Ô∏è *Recommendation:* Manual review suggested"
    
    # Confidence level with emoji
    conf_pct = result.confidence * 100
    if conf_pct >= 85:
        conf_emoji = "üü¢"
        conf_text = "Very High"
    elif conf_pct >= 70:
        conf_emoji = "üü°"
        conf_text = "High"
    elif conf_pct >= 55:
        conf_emoji = "üü†"
        conf_text = "Moderate"
    else:
        conf_emoji = "üî¥"
        conf_text = "Low"
    
    # Build WhatsApp message
    lines = [
        f"{verdict_emoji} *AI Text Analysis*",
        "",
        f"üìä *Result:* {verdict_text}",
        f"{conf_emoji} *Confidence:* {conf_text} ({conf_pct:.0f}%)",
        f"üó≥Ô∏è *Agreement:* {result.agreement}",
        "",
    ]
    
    # Add detector breakdown
    if result.breakdown:
        lines.append("üìã *Analysis Details:*")
        detector_names = {
            "detective": "DeTeCtive (Style)",
            "binoculars": "Binoculars (Stats)",
            "fast_detect": "Fast-GPT (Curve)"
        }
        for name, det in result.breakdown.items():
            icon = "üîµ" if det.prediction == "AI" else "‚ö™"
            display_name = detector_names.get(name, name)
            lines.append(f"  {icon} {display_name}: {det.prediction} ({det.score:.0%})")
        lines.append("")
    
    lines.append(action_line)
    
    message = "\n".join(lines)
    
    return {
        "success": True,
        "message": message,
        "prediction": result.prediction,
        "confidence": round(result.confidence, 4),
        "is_ai": result.prediction == "AI",
        "suggested_action": result.suggested_action
    }


def format_web_response(result, include_breakdown: bool = True) -> Dict[str, Any]:
    """Format detection result for web/API with full details."""
    
    response = {
        "success": True,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "result": {
            "prediction": result.prediction,
            "is_ai": result.prediction == "AI",
            "is_human": result.prediction == "Human",
            "is_uncertain": result.prediction == "UNCERTAIN",
            "confidence": round(result.confidence, 4),
            "confidence_percent": f"{result.confidence * 100:.1f}%",
            "ensemble_score": round(result.ensemble_score, 4),
            "agreement": result.agreement,
            "suggested_action": result.suggested_action,
            "action_color": {
                "REJECT": "#dc3545",
                "REVIEW": "#ffc107",
                "ACCEPT": "#28a745"
            }.get(result.suggested_action, "#6c757d")
        }
    }
    
    if include_breakdown and result.breakdown:
        response["breakdown"] = {}
        for name, det in result.breakdown.items():
            response["breakdown"][name] = {
                "prediction": det.prediction,
                "score": round(det.score, 4),
                "confidence": round(det.confidence, 4),
                "method": det.details.get("method", "Unknown")
            }
    
    return response


# =============================================================================
# Main Detector Class
# =============================================================================

@app.cls(
    image=image,
    gpu="T4",  # Cost-effective: ~$0.59/hr
    timeout=300,
    container_idle_timeout=300,  # 5 min idle before shutdown
    volumes={"/cache": model_volume},
)
class AITextDetector:
    """GPU-accelerated AI Text Detector."""
    
    detector = None
    start_time = None
    request_count = 0
    
    @modal.enter()
    def setup(self):
        """Initialize detector when container starts."""
        import sys
        import torch
        
        # Add ensemble to path
        sys.path.insert(0, "/root")
        
        # Set cache directories
        os.environ["HF_HOME"] = "/cache/huggingface"
        os.environ["TRANSFORMERS_CACHE"] = "/cache/huggingface"
        os.environ["TORCH_HOME"] = "/cache/torch"
        
        print("=" * 60)
        print("Initializing AI Text Detector on Modal...")
        print(f"PyTorch: {torch.__version__}")
        print(f"CUDA: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
        print("=" * 60)
        
        # Download and setup the ensemble code
        self._setup_ensemble()
        
        # Initialize detector
        from ensemble.detector import EnsembleDetector
        self.detector = EnsembleDetector(device="cuda")
        
        self.start_time = time.time()
        self.request_count = 0
        
        # Save models to persistent volume
        model_volume.commit()
        
        print("‚úÖ Detector ready!")
        print(f"   Weights: {self.detector.weights}")
    
    def _setup_ensemble(self):
        """Download and setup ensemble code from GitHub."""
        import subprocess
        
        # Clone the repository if not exists
        if not os.path.exists("/root/ensemble"):
            print("Downloading ensemble code...")
            
            # Clone just the ensemble directory
            subprocess.run([
                "git", "clone", "--depth", "1", "--filter=blob:none", "--sparse",
                "https://github.com/heyongxin233/DeTeCtive.git",
                "/tmp/repo"
            ], check=True)
            
            # Copy ensemble folder
            subprocess.run(["mkdir", "-p", "/root/ensemble"], check=True)
            
            # For now, we'll embed the essential code directly
            self._create_ensemble_code()
    
    def _create_ensemble_code(self):
        """Create ensemble code files directly (embedded for reliability)."""
        import subprocess
        
        # Create directory structure
        os.makedirs("/root/ensemble/core", exist_ok=True)
        os.makedirs("/root/ensemble/database/deepfake_sample", exist_ok=True)
        os.makedirs("/root/ensemble/models", exist_ok=True)
        
        # Create __init__.py
        with open("/root/ensemble/__init__.py", "w") as f:
            f.write('"""AI Text Detection Ensemble"""\n')
        
        with open("/root/ensemble/core/__init__.py", "w") as f:
            f.write('"""Core modules"""\n')
        
        # Download the actual detector code from the repo
        # For production, you'd want to include the full code here
        # or use a proper package installation
        
        print("Note: For production, ensure ensemble code is properly installed")
    
    @modal.method()
    def detect(
        self, 
        text: str, 
        format_type: str = "raw",
        include_breakdown: bool = True
    ) -> Dict[str, Any]:
        """
        Detect if text is AI-generated.
        
        Args:
            text: Text to analyze (20-10000 chars)
            format_type: "raw", "web", or "whatsapp"
            include_breakdown: Include per-detector results
            
        Returns:
            Detection result dictionary
        """
        self.request_count += 1
        
        # Validate input
        if not text or len(text.strip()) < 20:
            return {
                "success": False,
                "error": "Text too short",
                "detail": "Minimum 20 characters required for reliable detection",
                "text_length": len(text) if text else 0
            }
        
        # Truncate long texts
        truncated = False
        if len(text) > 10000:
            text = text[:10000]
            truncated = True
        
        try:
            # Run detection
            result = self.detector.detect(text, return_breakdown=include_breakdown)
            
            # Format response
            if format_type == "whatsapp":
                response = format_whatsapp_response(result)
            elif format_type == "web":
                response = format_web_response(result, include_breakdown)
            else:
                # Raw format
                response = {
                    "success": True,
                    "prediction": result.prediction,
                    "confidence": round(result.confidence, 4),
                    "ensemble_score": round(result.ensemble_score, 4),
                    "agreement": result.agreement,
                    "suggested_action": result.suggested_action,
                    "breakdown": {
                        name: {
                            "prediction": det.prediction,
                            "score": round(det.score, 4),
                            "confidence": round(det.confidence, 4)
                        }
                        for name, det in result.breakdown.items()
                    } if result.breakdown else {}
                }
            
            response["truncated"] = truncated
            return response
            
        except Exception as e:
            import traceback
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
                "traceback": traceback.format_exc()
            }
    
    @modal.method()
    def batch_detect(self, texts: List[str], format_type: str = "raw") -> Dict[str, Any]:
        """Batch detection for multiple texts (max 50)."""
        
        texts = texts[:50]  # Limit batch size
        results = []
        
        for text in texts:
            result = self.detect(text, format_type=format_type, include_breakdown=False)
            results.append(result)
        
        # Summary statistics
        successful = [r for r in results if r.get("success")]
        ai_count = sum(1 for r in successful if r.get("prediction") == "AI")
        human_count = sum(1 for r in successful if r.get("prediction") == "Human")
        
        return {
            "success": True,
            "results": results,
            "summary": {
                "total": len(texts),
                "processed": len(successful),
                "failed": len(texts) - len(successful),
                "ai_detected": ai_count,
                "human_detected": human_count,
                "uncertain": len(successful) - ai_count - human_count,
                "ai_percentage": f"{ai_count / len(successful) * 100:.1f}%" if successful else "0%"
            }
        }
    
    @modal.method()
    def health(self) -> Dict[str, Any]:
        """Health check with status information."""
        import torch
        
        uptime = time.time() - self.start_time if self.start_time else 0
        
        return {
            "status": "healthy",
            "version": "1.0.0",
            "model_loaded": self.detector is not None,
            "gpu_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            "uptime_seconds": round(uptime),
            "requests_handled": self.request_count,
            "detector_weights": self.detector.weights if self.detector else None,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }


# =============================================================================
# Web Endpoints
# =============================================================================

@app.function(image=image, gpu="T4", volumes={"/cache": model_volume})
@modal.web_endpoint(method="GET", docs=True)
def health() -> Dict[str, Any]:
    """Health check endpoint."""
    detector = AITextDetector()
    return detector.health.remote()


@app.function(image=image, gpu="T4", volumes={"/cache": model_volume})
@modal.web_endpoint(method="POST", docs=True)
def detect_endpoint(request: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main detection endpoint.
    
    Request body:
    {
        "text": "Text to analyze...",
        "format": "web" | "whatsapp" | "raw",
        "include_breakdown": true
    }
    """
    text = request.get("text", "")
    format_type = request.get("format", "web")
    include_breakdown = request.get("include_breakdown", True)
    
    detector = AITextDetector()
    return detector.detect.remote(text, format_type, include_breakdown)


@app.function(image=image, gpu="T4", volumes={"/cache": model_volume})
@modal.web_endpoint(method="POST", docs=True)
def whatsapp_endpoint(request: Dict[str, Any]) -> Dict[str, Any]:
    """
    WhatsApp bot endpoint - returns formatted message.
    
    Request body:
    {"text": "Text to analyze..."}
    
    Response includes a pre-formatted WhatsApp message with emojis.
    """
    text = request.get("text", request.get("message", ""))
    
    detector = AITextDetector()
    return detector.detect.remote(text, "whatsapp", True)


@app.function(image=image, gpu="T4", volumes={"/cache": model_volume})
@modal.web_endpoint(method="POST", docs=True)
def batch_endpoint(request: Dict[str, Any]) -> Dict[str, Any]:
    """
    Batch detection endpoint for multiple texts.
    
    Request body:
    {"texts": ["Text 1...", "Text 2...", ...]}
    """
    texts = request.get("texts", [])
    format_type = request.get("format", "raw")
    
    detector = AITextDetector()
    return detector.batch_detect.remote(texts, format_type)


# =============================================================================
# Local Testing
# =============================================================================

@app.local_entrypoint()
def main():
    """Local test entrypoint."""
    print("üß™ Testing AI Text Detector...")
    
    detector = AITextDetector()
    
    # Test texts
    ai_text = """Artificial intelligence has revolutionized various industries 
    by providing innovative solutions to complex problems. The implementation 
    of machine learning algorithms has enabled unprecedented levels of 
    automation and efficiency across multiple sectors."""
    
    human_text = """honestly idk why everyone's making such a big deal about 
    this whole AI thing lol... like yeah it's cool but my grandma still can't 
    figure out her iPhone so maybe let's calm down a bit? just saying ü§∑"""
    
    print("\n" + "=" * 60)
    print("üìù Testing AI-like text...")
    print("=" * 60)
    result = detector.detect.remote(ai_text, "whatsapp", True)
    print(result.get("message", result))
    
    print("\n" + "=" * 60)
    print("üìù Testing Human-like text...")
    print("=" * 60)
    result = detector.detect.remote(human_text, "whatsapp", True)
    print(result.get("message", result))
    
    print("\n" + "=" * 60)
    print("üíö Health check...")
    print("=" * 60)
    health_result = detector.health.remote()
    for key, value in health_result.items():
        print(f"  {key}: {value}")
